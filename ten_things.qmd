---
title: "Ten Things I Love About R"
format: 
  gfm: 
    toc: true
editor: visual
---

# The tidyverse

When I first started using R, there was no [tidyverse](https://www.tidyverse.org). There was just base R, which caused me heartburn.

But the tidyverse changed all that. It makes using R a breeze.

The tidyverse has packages for [reading data](https://readr.tidyverse.org), [manipulating data](https://dplyr.tidyverse.org), [cleaning data](https://tidyr.tidyverse.org) and many other things.

You can install them with just a simple command.

```{r}
#install.packages(tidyverse)
```

Then you can load all the packages at once:

```{r}
library(tidyverse)
```

The people behind the tidyverse believe in what they call tidy data. It's a concept we use every single day.

Each column is a variable. Each record is an observation. Every cell is a single value.

Those concepts flow through other packages not in the tidyverse, but adhere to tidy data principles.

For example, [sf](https://r-spatial.github.io/sf/) is a GIS package that adheres to tidy principles, which you'll see below. For rasters, there's [stars](https://r-spatial.github.io/stars/) package that's meant for spatio-temporal arrays.

For machine learning/modelling, there's [tidymodels](https://www.tidymodels.org).

One other thing. You'll notice a number of R packages, particularly in the tidyverse, have these

# Import Data

Importing data is a pain in the butt. I think R makes some of those tasks easier.

You can deal with lots of

For example, if I need to get data into Postgres or SQL Server, I'll use R as an intermediary.

I can use the tidyverse's [readr](https://readr.tidyverse.org).

```{r}
single_dataset <- read_csv("./data/importing/abw.csv")

single_dataset
```

You'll see there are other [readr functions](https://readr.tidyverse.org/reference/index.html) to help you import data.

Sidenote: Have a SAS, SPSS or Stata dataset? You can import it with [haven](https://haven.tidyverse.org).

One thing I like to do is import everything as character, then later convert to appropriate data types. I can do that easily with [read_csv](https://readr.tidyverse.org/reference/read_delim.html).

```{r}
single_dataset <- read_csv("./data/importing/abw.csv", col_types = cols(.default = "c"))
```

# FS

OK. Say you have a directory of files. You want to import them all into a single dataset.

There's a tidy-friendly package called [fs](https://fs.r-lib.org) for working with file systems.

You can install it like this:

```{r}
#install.packages("fs")
```

Ok back to the topic at hand.

Let's initialize fs.

```{r}
library(fs)
```

What's in our directory? Annoyingly, there's a different file for every country. What to do?

```{r}
dir_ls(path="./data/importing")
```

fs makes it easy.

```{r}
imported_data_collection <- dir_ls(path="./data/importing") %>%
  map_df(read_csv, .id = "file", col_types = cols(.default = "c")) # map applies a function to each record...so if each filename from dir_ls is a record, this applies read_csv to each record. 

imported_data_collection
```

And, take note how it also includes the filename from which that data originated. Useful.

# Clean column names

I use a single function from the [janitor package](https://sfirke.github.io/janitor/index.html). More precisely: I use [clean_names](https://sfirke.github.io/janitor/reference/clean_names.html) constantly.

The data we just imported gives us a perfect use case. Here are our ugly column names. Column names starting with numbers? Unacceptable. 

```{r}
colnames(imported_data_collection)
```

Yuck. But we can use clean_names:

```{r}
imported_data_collection_fixed <- imported_data_collection %>%
  janitor::clean_names() # Notice here that I used the library name and function name. You can do this if you don't want to load the entire package into memory. 

imported_data_collection_fixed
```

# Work with dates

```{r}
library(nycflights13)
library(lubridate)
```

We can use lubridate to do some nice things.

```{r}
flights %>% 
  select(year, month, day, hour, minute) %>% 
  mutate(departure = make_datetime(year, month, day, hour, minute))
```

We can easily extract elements of the date.

```{r}
flights %>% 
  select(year, month, day, hour, minute) %>% 
  mutate(departure = make_datetime(year, month, day, hour, minute)) %>%
  select(departure) %>%
  mutate(the_year = year(departure),
         the_day = day(departure))
```

And then we can do all kinds of things with differences in times.

```{r}
make_datetime_100 <- function(year, month, day, time) {
  make_datetime(year, month, day, time %/% 100, time %% 100)
}

flights_dt <- flights %>% 
  filter(!is.na(dep_time), !is.na(arr_time)) %>% 
  mutate(
    dep_time = make_datetime_100(year, month, day, dep_time),
    arr_time = make_datetime_100(year, month, day, arr_time),
    sched_dep_time = make_datetime_100(year, month, day, sched_dep_time),
    sched_arr_time = make_datetime_100(year, month, day, sched_arr_time)
  ) %>% 
  select(origin, dest, ends_with("delay"), ends_with("time"))

flights_dt
```

You can do simple date comparisons.

```{r}
flights_dt %>% 
  mutate(late_departure = difftime(ymd_hms(sched_dep_time), ymd_hms(dep_time), units="secs"))
```

# Interact with databases

OK. I've imported my data and I want to send it to Postgres.

[RPostgres](https://rpostgres.r-dbi.org) makes that easy.

First, if needed, install via:

```{r}
#install.packages("RPostgres")
```

To connect with the DB, you first need to set up a little connection string to the DB. If you've ever used ODBC, it's kinda similar to that process.

Here's one for a local Postgres install. Notice I used the askForPassword function. That's so I don't encode my username and password when I plan to upload this to Github.

```{r}
library(RPostgres)

con <- dbConnect(RPostgres::Postgres(),
                 dbname = 'test2', 
                 host = "localhost", 
                 port = 5432, # or any other port specified by your DBA
                 user = "rstudio", 
                 password = "baseball1"
)

```

From here, you can do [all kinds of things](https://rpostgres.r-dbi.org/reference/index.html).

Want to send our dataset to the DB?

```{r}
dbWriteTable(con, "my_table_name", imported_data_collection_fixed, overwrite=TRUE)
```

Want to read it back?

```{r}
showing_off <- dbReadTable(con, "my_table_name")

glimpse(showing_off)
```

What if you want to write your own SQL?

You can do it a couple ways.

```{r}
# first way
showing_off <- dbGetQuery(con, "select indicator, x2000 from my_table_name")

glimpse(showing_off)
```

Second way.

```{r}
results <- dbSendQuery(con, "select indicator, x2000 from my_table_name")
dbFetch(results, n=100) 
```

But wait. There's more. Entering the chat now is [dbplyr](https://dbplyr.tidyverse.org), which uses dplyr verbs to interact with a database. Sweet.

```{r}
library(dbplyr)
```

Now say you want to interact with a particular table.

First create a connection to that table like so:

```{r}
my_dbplyr_connection <- tbl(con, "my_table_name")

my_dbplyr_connection
```

To me, one of the real time-saving features of dbplyr is how it makes select statements much easier.

Say you just want to extract the total population for each year--- you can use [tidyselectors](https://tidyselect.r-lib.org/reference/language.html) that make it easier than writing a traditional select statement in SQL.

```{r}
my_dbplyr_connection %>%
  filter(indicator=="SP.POP.TOTL") %>%
  select(file, starts_with("x2")) # saves me writing out all those field names in a select statement
```

# Wrangling data

OK. One thing we need to add is extract the ISO3 code of the country buried in the filename. We can extract those from the file name using [str_extract](https://stringr.tidyverse.org/reference/str_extract.html) and [regular expressions](https://cran.r-project.org/web/packages/stringr/vignettes/regular-expressions.html).

```{r}
my_pattern <- "./data/importing/(\\D\\D\\D).csv"

imported_data_collection_fixed_2 <- imported_data_collection_fixed %>%
  filter(indicator=="SP.POP.TOTL") %>%
  mutate(country_code = str_extract(file, my_pattern, group=1)) %>%
  select(file, country_code, starts_with("x2"))

imported_data_collection_fixed_2
```

Cool huh?

# Pivot Data

OK. So remember how we imported all that data as character? We need to convert all those pop fields to integers.

We could write a mutate statement to do each of the years individually. Or we could pivot the data, making a wide dataset into a long one.

```{r}
imported_data_collection_fixed_2 %>%
  pivot_longer(cols=starts_with("x"), names_to = "the_year", values_to = "pop_tot")
```

So now we can just convert that one field to numeric, then pivot it back. 

```{r}
imported_data_collection_fixed_3 <- imported_data_collection_fixed_2 %>%
  pivot_longer(cols=starts_with("x"), names_to = "the_year", values_to = "pop_tot") %>%
  mutate(pop_tot_num = as.numeric(pop_tot)) %>%
  select(!pop_tot) %>% # drop the old character field
  pivot_wider(names_from = the_year, values_from = pop_tot_num)

imported_data_collection_fixed_3
```

Voila!

# Easy charting

So let's take our last dataset and do some charting. How about we chart China and India's population?

We're going to filter for those two countries, then pivot longer because it's a time series, then we're going to turn our year field name into an integer. 

```{r}
ready_to_chart <- imported_data_collection_fixed_3 %>%
  filter(country_code %in% c("IND", "CHN")) %>%
  pivot_longer(cols = starts_with("x"), names_to = "the_year", values_to = "pop") %>%
  mutate(the_year = as.integer(str_remove(the_year, "x")))

ready_to_chart
```

```{r}
p1 <- ggplot(data=ready_to_chart, aes(x=the_year, y=pop, group=country_code, color=country_code)) + 
  geom_line() +
  geom_point()

p1
```


# Do GIS stuff

OK. So we've all traditionally worked with desktop applications, such as QGIS or ArcGIS.

We can do all of that stuff in R. 

For vectors, we can use SF. For rasters, we can use stars. 

```{r}
#install.packages("stars")
#install.packages("sf")
```

```{r}
library(stars)
library(sf)
```

Let's say we have a raster of 2020 population density in Switzerland. We can read it into a stars object like so. 

```{r}
swiss_raster <- read_stars("./data/che_pd_2020_1km.tif")
```

Let's take a look. 

```{r}
ggplot() + 
  geom_stars(data=swiss_raster)
```

Let's say we wanted to see just the area around Zurich. 

One simple way is to use [tribble](https://tibble.tidyverse.org/reference/tribble.html), which makes it easy to create an SF point on the fly. Let's create a dot for Zurich.

```{r}
zurich_dot <- tribble(
  ~site, ~x, ~y,
  "Zurich", 8.541111, 47.374444
  ) %>%
  st_as_sf(coords = c("x", "y"), crs = 4326)
```

Let's make sure the dot is where the dot is supposed to be --- in Zurich. 

```{r}
ggplot() + 
  geom_stars(data=swiss_raster) +
  geom_sf(data=zurich_dot, color="red")
```

Now we've got a dot. Let's buffer that dot.

```{r}
zurich_25k_buffer <- zurich_dot %>%
  st_buffer(25000) # base unit in meters, so 25,000m = 25km
```

Let's see how it looks.

```{r}
ggplot() + 
  geom_stars(data=swiss_raster) +
  geom_sf(data=zurich_dot, color="red") +
  geom_sf(data=zurich_25k_buffer, fill=NA, color="red")
```

Now let's crop the raster by our buffer. 

```{r}
swiss_raster_cropped <- swiss_raster %>%
  st_crop(zurich_25k_buffer) 
```

How does it look?

```{r}
ggplot() + 
  geom_stars(data=swiss_raster_cropped)
```

I won't go too far down the rabbit hole, but you can use [tidyverse methods](https://r-spatial.github.io/stars/articles/stars3.html) with stars objects. 

Here we rename the attribute to pop_density_20, then we create a new band with a yes or no whether it's high density. Then we select just our newly created high_density band. 

```{r}
new_band_example <- swiss_raster_cropped %>%
  rename(pop_density_20 = 1) %>%
  mutate(high_density = if_else(pop_density_20 > 3000, "Yes", "No")) %>%
  select(high_density) 
```

How does it look?

```{r}
ggplot() + 
  geom_stars(data=new_band_example)
```

What if you're like me and prefer to work in vectors?

Let's return to our swiss_raster_cropped stars object. 

It takes just one line of code to turn this raster into an SF vector object. 

```{r}
swiss_raster_cropped_sf <- swiss_raster_cropped %>%
  st_as_sf()
```

Now let's take a look. 

```{r}
ggplot() + 
  geom_sf(data=swiss_raster_cropped_sf)
```


What about a spatial filter?

Let's create another smaller buffer as an example. 

```{r}
zurich_10k_buffer <- zurich_dot %>%
  st_buffer(10000) 
```

Now we can use st_filter. 

```{r}
smaller_zurich_area <- swiss_raster_cropped_sf %>%
  st_filter(zurich_10k_buffer)
```

How does it look?

```{r}
ggplot() + 
  geom_sf(data=swiss_raster_cropped_sf) +
  geom_sf(data=smaller_zurich_area, color="red")
```


